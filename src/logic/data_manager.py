import os
import shutil
import time
# ‚úÖ ‡πÉ‡∏ä‡πâ FAISS ‡πÅ‡∏ó‡∏ô Chroma
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_core.documents import Document 

PERSIST_DIRECTORY = "db_storage"
RULE_SEPARATOR = "--------------------"

def rebuild_knowledge_base(folder_path="knowledge_base"):
    """‡∏•‡πâ‡∏≤‡∏á‡∏™‡∏°‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ FAISS"""
    
    # 1. ‡∏•‡πâ‡∏≤‡∏á DB ‡πÄ‡∏Å‡πà‡∏≤
    if os.path.exists(PERSIST_DIRECTORY):
        try:
            shutil.rmtree(PERSIST_DIRECTORY)
            time.sleep(1)
        except:
            pass 

    documents = []
    
    # 2. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå .txt ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    if not os.path.exists(folder_path): os.makedirs(folder_path)
    
    files = os.listdir(folder_path)
    if not files: return "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå knowledge_base"

    for f_name in files:
        f_path = os.path.join(folder_path, f_name)
        if f_name.endswith(".txt"):
            try:
                with open(f_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    rules = content.split(RULE_SEPARATOR)
                    for r in rules:
                        if r.strip():
                            documents.append(Document(page_content=r.strip(), metadata={"source": f_name}))
            except:
                pass 

    if not documents: return "‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏é‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .txt"

    # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector DB ‡∏î‡πâ‡∏ß‡∏¢ FAISS üöÄ
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        vector_db = FAISS.from_documents(documents, embeddings)
        vector_db.save_local(PERSIST_DIRECTORY)
        
        return f"‚úÖ ‡∏à‡∏î‡∏à‡∏≥‡∏Å‡∏é‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(documents)} ‡∏Ç‡πâ‡∏≠ (‡∏£‡∏∞‡∏ö‡∏ö FAISS)"
    except Exception as e:
        return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á DB: {str(e)}"

def search_rules(query):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"""
    if not os.path.exists(PERSIST_DIRECTORY): return ""
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ allow_dangerous_deserialization=True)
        vector_db = FAISS.load_local(PERSIST_DIRECTORY, embeddings, allow_dangerous_deserialization=True)
        
        results = vector_db.similarity_search(query, k=5)
        
        if not results: return ""
        
        return "\n\n--------------------\n\n".join([doc.page_content for doc in results])
    except Exception as e:
        print(f"Search Error: {e}")
        return ""